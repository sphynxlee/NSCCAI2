
 Saved

Listen
What type of learning algorithm is KNN (K Nearest Neighbours)?

Question 1 options:

unsupervised


supervised


semi-supervised


reinforcement learning

Question 2 (1 point)
 Saved

Listen
How does the KNN algorithm classify a new data point?

Question 2 options:

By assigning the majority class among its k nearest neighbors


By calculating the mean value of its k nearest neighbors


By performing gradient descent on its k nearest neighbors


By randomly selecting the class label from its k nearest neighbors

Question 3 (1 point)
 Saved

Listen
In the KNN algorithm, what does "k" represent?

Question 3 options:

The number of features in the dataset


The distance metric used to measure similarity


The number of nearest neighbors to consider


The dimensionality of the dataset

Question 4 (1 point)
 Saved

Listen
What type of algorithm is K-Means clustering?

Question 4 options:

Reinforcement learning


Unsupervised learning


Supervised learning


Semi-supervised learning

Question 5 (1 point)
 Saved

Listen
How does the K-Means algorithm initialize cluster centroids?

Question 5 options:

Randomly selecting data points from the dataset


Using the mean of the entire dataset


Manually specifying initial centroid positions


Using a predefined set of points as centroids

Question 6 (1 point)
 Saved

Listen
What is the objective function optimized by the K-Means algorithm?

Question 6 options:

Mean squared error


Silhouette coefficient


Daviesâ€“Bouldin index


Entropy

Question 7 (1 point)
 Saved

Listen
What does a decision tree node represent?

Question 7 options:

A decision boundary separating classes


A feature or attribute of the dataset


A cluster of similar data points


Decision points where the tree splits into branches

Question 8 (1 point)
 Saved

Listen
Entropy is commonly used as a metric for decision tree training.

Question 8 options:
	True
	False
Question 9 (1 point)
 Saved

Listen
Which of the following metrics did we use for distance measuring in our KNN example?

Question 9 options:

The Manhattan metric


The Euclidean metric


The Minkowski metric


Cosine Similarity

Question 10 (1 point)
 Saved

Listen
The k in KNN and k-Means Clustering represent the same thing

Question 10 options:
	True
	False
Question 11 (1 point)
 Saved

Listen
What are the centers of the clusters in K-Means called?

Question 11 options:

midpoints


centroids


locus points


center of mass points

Question 12 (1 point)
 Saved

Listen
KNN can only be used for categorization problems.

Question 12 options:
	True
	False
Question 13 (1 point)
 Saved

Listen
What machine learning Python library did we use to implement K-Means Clustering?

Question 13 options:

PyCaret


Keras


Scikit-learn


Tensorflow

Question 14 (1 point)
 Saved

Listen
Which of the following is not a primary topic you need to study to understand transformers?

Question 14 options:

Attention


Word Embeddings


Gates


Encoder / Decoders

Question 15 (1 point)
 Saved

Listen
Which of the following is not a matrix used in calculating attention in transformers?

Question 15 options:

Query


Key


Convolution


Value

Question 16 (1 point)
 Saved

Listen
Transformers make use of the SoftMax function.

Question 16 options:
	True
	False
Question 17 (1 point)
 Saved

Listen
Transformers rely on a single hidden state to encompass all the information about a word sequence.

Question 17 options:
	True
	False
Question 18 (1 point)
 Saved

Listen
What is the particular type of attention used in transformers?

Question 18 options:

Multi-headed, self-attention


vanilla, basic attention


external attention


cosine attention

Question 19 (1 point)
 Saved

Listen
What type of encoding is done to capture word order?

Question 19 options:

Attentional


Relational


Positional


Ordinal

Question 20 (1 point)
 Saved

Listen
What was the first backend for Keras?

Question 20 options:

PyTorch


TinyGrad


Theano


TensorFlow

Question 21 (1 point)
 Saved

Listen
TensorFlow is older than Torch/PyTorch.

Question 21 options:
	True
	False
Question 22 (1 point)
 Saved

Listen
JAX is best described as

Question 22 options:

a fully featured machine learning framework


a framework focused only on deep neural networks


a foundational framework layer for numerical computation


the next version of TensorFlow

Question 23 (1 point)
 Saved

Listen
TensorFlow is still more used/popular than PyTorch.

Question 23 options:
	True
	False
Question 24 (1 point)
 Saved

Listen
Which company ported Torch from Lua to Python, creating PyTorch?

Question 24 options:

Microsoft


Google


Facebook/Meta


Netflix

Question 25 (1 point)
 Saved

Listen
In one-hot encoding, you translate symbols into vectors. What is the numeric value of most of the components/elements in those vectors?

Question 25 options:

1


0


-1


the values are not numeric

Question 26 (1 point)
 Saved

Listen
When using one-hot encoding, two different symbols could potentially have the same representation/encoding.

Question 26 options:
	True
	False
Question 27 (1 point)
 Saved

Listen
What is the maximum value of the dot product of any 2 (not necessarily distinct) one-hot vectors?

Question 27 options:

the length of the vectors


0


1


there is no maximum value

Question 28 (1 point)
 Saved

Listen
Taking the dot product is an integral part of what other mathematical operation on matrices?

Question 28 options:

matrix subtraction


matrix multiplication


matrix trace


matrix addition

Question 29 (1 point)
 Saved

Listen
What property is satisfied if the next word you will encounter in a sentence is only dependent upon the word preceding it.

Question 29 options:

The Cauchy Property


The Minkowski Property


The Markov Property


The Langlands Property

Question 30 (1 point)
 Saved

Listen
Markov chains cannot be expressed in matrix form.

Question 30 options:
	True
	False
Question 31 (1 point)
 Saved

Listen
Like in any other Neural Network, transformer parameters need to be learned. This is done using backpropagation and relies upon the functional relationship between parameters and model error (or loss) being smooth: each computation step must be _____________________.

Question 31 options:

algorithmically efficient


Turing computable


differentiable


atomic

Question 32 (1 point)
 Saved

Listen
Embeddings typically take up more space than one-hot encoding.

Question 32 options:
	True
	False
Question 33 (1 point)
 Saved

Listen
What happens right after Input Embedding in a transformer?

Question 33 options:

Softmax


A linear layer


Add & Norm


Positional Encoding

Question 34 (1 point)
 Saved

Listen
Which of the following is not a property of the Softmax function?

Question 34 options:

emphasizes the highest value


forces all values to lie in [0,1] range


can not be differentiated


de-emphasizes lowest values

Question 35 (1 point)
 Saved

Listen
Which architectural component of a transformer ensures that it cannot look into the future when doing sequence generation?

Question 35 options:

argmax


norm


concat


mask

Question 36 (1 point)
 Saved

Listen
The softmax function tends to focus on one element - and this can be a bit of a problem when applied to the Attention mechanism. We get around this by using:

Question 36 options:

Q, K, and V matrices


multi-headed attention


self-attention


argmax

Question 37 (1 point)
 Saved

Listen
Attention acts like a filter, blocking most of what tries to pass through it.

Question 37 options:
	True
	False
Question 38 (1 point)
 Saved

Listen
Which of the following is a technique that maintains a consistent distribution of signal values each step of the way throughout many-layered neural networks.

Question 38 options:

gradient descent


normalization


concatenation


skip connections

Question 39 (1 point)
 Saved

Listen
What is the mechanism that provides the connection between the encoder and decoder stacks of a transformer?

Question 39 options:

tokenizing


cross-attention


byte pair encoding


skip connections

Question 40 (1 point)
 Saved

Listen
What property of matrices shows up in the name LoRA?

Question 40 options:

order


limit


rank


associativity

Question 41 (1 point)
 Saved

Listen
LLM fine tuning updates the weights of a base LLM, whereas RAG'ing provides additional information that can be queried and prompt-injected.

Question 41 options:
	True
	False
Question 42 (1 point)
 Saved

Listen
What does the Q in QLoRA stand for?

Question 42 options:

Query-localized


Quick


Quantized


Quiet

Question 43 (1 point)
 Saved

Listen
What is FAISS primarily used for?

Question 43 options:

Natural language processing


Computer vision tasks


Speech recognition


Similarity search in high-dimensional vector spaces

Question 44 (1 point)
 Saved

Listen
What is the primary purpose of random feature selection in Random Forests?

Question 44 options:

To reduce the number of features in the dataset


To increase the computational efficiency of training


To introduce diversity among the trees


To improve the accuracy of individual trees

Question 45 (1 point)
 Saved

Listen
Bagging (Bootstrap Aggregating) helps to reduce variance and improve the stability of a tree-based model. It also contributes to the decorrelation of the trees by exposing each tree to slightly different subsets of the data.

Question 45 options:
	True
	False
Question 46 (1 point)
 Saved

Listen
A client asks you which of the following ML libraries is likely to provide the best performance. Which would you choose?

Question 46 options:

TensorFlow


PyTorch


ML.net


JAX

Question 47 (1 point)
 Saved

Listen
When you here about Llama2B or Falcon180B what are those billions referring to? Billions of what?

Question 47 options:

CPU cycles used to train


connections


inputs


weights

Question 48 (1 point)
 Saved

Listen
What is Q-learning primarily used for?

Question 48 options:

clustering


reinforcement learning


regression


classification

Question 49 (1 point)
 Saved

Listen
In Q learning the 'Q' stands for quality.

Question 49 options:
	True
	False
Question 50 (1 point)
 Saved

Listen
Which algorithm is Q-learning based on?

Question 50 options:

Gradient Descent


k-Nearest Neighbours


The Bellman Equation


Markov Decision Processes